{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wDQ__M8r9VZH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('/content/data.txt','r',errors='ignore')\n",
        "raw_doc=f.read()"
      ],
      "metadata": {
        "id": "qu6sJWug9mbi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Preprocessing in raw_doc"
      ],
      "metadata": {
        "id": "KiaF6qqE92Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_doc=raw_doc.lower()   ## converting the entire text to lowercase\n",
        "nltk.download('punkt')    ## using the Punkt tokenizer\n",
        "nltk.download('wordnet')  ## using the wordnet dictionary\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYSq3Qwq9yUs",
        "outputId": "19994fd2-836d-40ed-a273-0fdd8e5e0761"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens=nltk.sent_tokenize(raw_doc)\n",
        "word_tokens=nltk.word_tokenize(raw_doc)"
      ],
      "metadata": {
        "id": "PVwtfyzL97uP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After Tokenization"
      ],
      "metadata": {
        "id": "3S3W5VQ6-DY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LunMe9mv9-y7",
        "outputId": "80d22b5c-6819-4f3a-93f7-20238a647b70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the reason given is: this article is using citations from 1970 and virtually all claims about conversational capabilities are at least ten years out of date (for example the turing test was arguably made obsolete years ago by transformer models).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ri682dB0-Hqe",
        "outputId": "9c507a08-671a-4d96-f337-0ddaef0c2237"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wikipediathe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Text-Processing Steps"
      ],
      "metadata": {
        "id": "3pOpk7mr-SnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punc_dict=dict((ord(punct),None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n"
      ],
      "metadata": {
        "id": "qlhhisE6-OoC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Define Greeting Function\n",
        "\n"
      ],
      "metadata": {
        "id": "YrOVrfnp-aku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greet_inputs= ('hello','hi','whassup','how are you?')\n",
        "greet_responses =('Hi','Hey','Hey There!','Hello')\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)"
      ],
      "metadata": {
        "id": "y9L0flJT-XwZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response Generation by Bot"
      ],
      "metadata": {
        "id": "VQ7lG51d-hj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "MAXjAcWA-fIZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "  robo1_response =''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer =LemNormalize,stop_words='english')\n",
        "  tfidf=TfidfVec.fit_transform(sentence_tokens)\n",
        "  vals=cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx=vals.argsort()[0][-2]\n",
        "  flat=vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf=flat[-2]\n",
        "  if(req_tfidf==0):\n",
        "    robo1_response = robo1_response + \"I an sorry. Unable to understand you\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response = robo1_response + sentence_tokens[idx]\n",
        "    return robo1_response"
      ],
      "metadata": {
        "id": "1b0QZgc_-mO-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the ChatFlow"
      ],
      "metadata": {
        "id": "NKOTOHT_-voH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag=True\n",
        "print('Hello! I am the Retreival Learning Bot. Start typing your text after greeting to talk to me.For ending covo typr bye!')\n",
        "while (flag==True):\n",
        "  user_response=input()\n",
        "  user_response=user_response.lower()\n",
        "  if(user_response != 'bye'):\n",
        "    if(user_response =='thank you' or user_response=='thanks'):\n",
        "      flag=False\n",
        "      print('Bot: You are Welcome..')\n",
        "    else:\n",
        "      if(greet(user_response)!= None):\n",
        "        print('Bot '+ greet(user_response))\n",
        "      else:\n",
        "        sentence_tokens.append(user_response)\n",
        "        word_tokens= word_tokens + nltk.word_tokenize(user_response)\n",
        "        final_words=list(set(word_tokens))\n",
        "        print('Bot: ',end ='')\n",
        "        print(response(user_response))\n",
        "        sentence_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag=False\n",
        "    print('Bot: GoodBye!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7as0HB83-tcU",
        "outputId": "a6fc0d5e-ba1e-4f54-d154-f3e16fe9d9ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am the Retreival Learning Bot. Start typing your text after greeting to talk to me.For ending covo typr bye!\n",
            "hi\n",
            "Bot Hey\n",
            "chatbot\n",
            "Bot: \"dbpedia chatbot\".\n",
            "the most notable early chatbots\n",
            "Bot: development\n",
            "among the most notable early chatbots are eliza (1966) and parry (1972).\n",
            "applications\n",
            "Bot: in banking, their major application is related to quick customer service answering common requests, as well as transactional support.\n",
            "application in messaging app\n",
            "Bot: application\n",
            "see also: virtual assistant\n",
            "messaging apps\n",
            "many companies' chatbots run on messaging apps or simply via sms.\n",
            "chatbot jobs\n",
            "Bot: [80]\n",
            "\n",
            "chatbot jobs\n",
            "\n",
            "chatbot developers create, debug, and maintain applications that automate customer services or other communication processes.\n",
            "limitations of a chatbot\n",
            "Bot: [76][77]\n",
            "\n",
            "limitations of chatbots\n",
            "the creation and implementation of chatbots is still a developing area, heavily related to artificial intelligence and machine learning, so the provided solutions, while possessing obvious advantages, have some important limitations in terms of functionalities and use cases.\n",
            "thank you for the information\n",
            "Bot: information.\n",
            "thank u\n",
            "Bot: I an sorry. Unable to understand you\n",
            "thanks\n",
            "Bot: You are Welcome..\n"
          ]
        }
      ]
    }
  ]
}